[toc]



```python
P9 系列三 线性回归1-最小二乘法及其几何意义
```

# 线性回归

假设数据集为：
$$
\mathcal{D}=\{(x_1, y_1),(x_2, y_2),\cdots,(x_N, y_N)\}
$$
后面我们记：
$$
X=(x_1,x_2,\cdots,x_N)^T,Y=(y_1,y_2,\cdots,y_N)^T
$$
线性回归假设：
$$
f(w)=w^Tx
$$

## 一、最小二乘法

对这个问题，采用二范数定义的平方误差来定义损失函数：
$$
L(w)=\sum\limits_{i=1}^N||w^Tx_i-y_i||^2_2
$$
### 1.1 范数的概念

范数（norm）是数学中的一种基本概念，特别在泛函分析、线性代数及相关领域中扮演着重要角色。简单来说，范数是一种“长度”或“大小”的度量方式，用于量化向量空间（或矩阵）中元素的大小或长度。它通常满足以下三个条件：

1. **非负性**：范数总是非负的。
2. **齐次性**：对于任何标量k和向量v，有||kv|| = |k|||v||。
3. **三角不等式**：对于任何两个向量u和v，有||u + v|| ≤ ||u|| + ||v||。

在二维的欧氏几何空间中，元素被表示为从原点出发的带有箭头的有向线段，而每一个矢量的有向线段的长度即为该矢量的欧氏范数。范数不仅限于欧氏空间，还广泛应用于各种向量空间和矩阵上，以度量其大小或复杂度。

$$
||x||_2 = \sqrt{\sum_{i=1}^{n} x_i^2}
$$

 

- ==L2范数在数学和机器学习中经常被用作一种正则化项、距离度量或误差度量==。例如，在机器学习的损失函数中引入L2范数作为正则项，**可以有效防止模型过拟合，提高模型的泛化能力**。

 



(4) 展开得到：  下面第二行四项都是1*1 都是标量，可以装置，并且合并。
$$
\begin{align}
L(w)&=(w^Tx_1-y_1,\cdots,w^Tx_N-y_N)\cdot (w^Tx_1-y_1,\cdots,w^Tx_N-y_N)^T\nonumber\\
&=(w^TX^T-Y^T)\cdot (Xw-Y)=w^TX^TXw-Y^TXw-w^TX^TY+Y^TY\nonumber\\
&=w^TX^TXw-2w^TX^TY+Y^TY
\end{align}
$$
> 在矩阵分析中，经常需要将传统的矩阵求导转换为对迹（trace）的求导，因为迹的求导在处理矩阵方程时往往更为简洁和直观。迹（trace）定义为矩阵对角线上元素的和，即对于矩阵$A \in \mathbb{R}^{n \times n}$，有

$$ \text{tr}(A) = \sum_{i=1}^{n} A_{ii} $$

> 以下是一些常见的矩阵求导公式及其转换为对迹求导的形式：

1. <font color=red>线性项</font>

对于 $ \frac{d}{dX} \text{tr}(AX) $，其中 $ A $ 和 $ X $ 是矩阵，且 $ A $ 不依赖于 $ X $。

- **传统求导**：直接得到 $ A^T $（假设 $ A $ 和 $ X $ 是同型矩阵）。
- **对迹求导**：
  $$ \frac{d}{dX} \text{tr}(AX) = \frac{d}{dX} \sum_{i,j} A_{ij}X_{ji} = \sum_{i,j} A_{ij} \frac{d}{dX_{ji}} X_{ji} = \sum_{i,j} A_{ij} I_{ji} = A^T $$
  这里 $ I $ 是单位矩阵，其元素 $ I_{ij} = 1 $ 当 $ i = j $，否则为 0。

2. <font color=red>二次型</font>

对于 $ \frac{d}{dX} \text{tr}(X^TAX) $，其中 $ A $ 是对称矩阵。

- **传统求导**：得到 $ 2AX $（假设 $ A $ 是对称的）。
- **对迹求导**：
  $$ \frac{d}{dX} \text{tr}(X^TAX) = \frac{d}{dX} \sum_{i,j,k} X_{ki}A_{ij}X_{jk} = \sum_{i,j,k} A_{ij} \left( \frac{d}{dX_{ki}} X_{ki} X_{jk} + X_{ki} \frac{d}{dX_{jk}} X_{jk} \right) $$
  $$ = \sum_{i,j,k} A_{ij} \left( I_{ki} X_{jk} + X_{ki} I_{jk} \right) = \sum_{i,j} A_{ij} \left( X_{ji} + X_{ij} \right) = 2 \sum_{i,j} A_{ij} X_{ji} = 2AX $$
  注意这里利用了 $ A $ 的对称性 $ A_{ij} = A_{ji} $。

<font color=red>3. 链式法则</font>

对于复合函数，如 $ \frac{d}{dX} \text{tr}(f(X)g(X)) $，其中 $ f(X) $ 和 $ g(X) $ 是矩阵函数。

- **对迹求导**：
  $$ \frac{d}{dX} \text{tr}(f(X)g(X)) = \text{tr} \left( \frac{df(X)}{dX} g(X)^T + f(X) \frac{dg(X)}{dX}^T \right) $$
  这里用到了==矩阵微分==的链式法则和迹的交换性质 $\text{tr}(AB) = \text{tr}(BA)$。

结论

将矩阵求导转换为对迹的求导，关键在于利用迹的线性性质和交换性质，以及矩阵微分的链式法则。这种方法在处理复杂的矩阵方程和优化问题时特别有用。

### 1.2 最小二乘法的推导





最小化这个值的 $ \hat{w}$​ ：
$$
\begin{align}
\hat{w}=\mathop{argmin}\limits_wL(w)&\longrightarrow\frac{\partial}{\partial w}L(w)=0\nonumber\\
&\longrightarrow2X^TX\hat{w}-2X^TY=0\nonumber\\
&\longrightarrow \hat{w}=(X^TX)^{-1}X^TY=X^+Y
\end{align}
$$
这个式子中 $(X^TX)^{-1}X^T$ 又被称为伪逆。对于行满秩或者列满秩的 $X$，可以直接求解，但是对于非满秩的样本集合，需要使用奇异值分解（SVD）的方法，对 $X$ 求奇异值分解，得到
$$
X=U\Sigma V^T
$$
于是：
$$
X^+=V\Sigma^{-1}U^T
$$
在几何上，最小二乘法相当于模型（这里就是直线）和试验值的距离的平方求和，假设我们的试验样本张成一个 $p$ 维空间（满秩的情况）：$X=Span(x_1,\cdots,x_N)$，而模型可以写成 $f(w)=X\beta$，也就是 $x_1,\cdots,x_N$ 的某种组合，而最小二乘法就是说希望 $Y$ 和这个模型距离越小越好，于是它们的差应该与这个张成的空间垂直：
$$
X^T\cdot(Y-X\beta)=0\longrightarrow\beta=(X^TX)^{-1}X^TY
$$

- 就是向量个数小于它的维度，没办法张成完整的向量空间  N>P
- X左乘，可以看作X列向量的线性组合
- 下面右下角给出几何推导

![image-20240901152345224](/Users/zhihongli/Documents/Course/MachineLearningNotes-master/pic/image-20240901152345224.png)

### 1.3 几何意义

在线性回归中，W权重向量的几何意义可以从多个方面来理解：



一、权重向量的基本定义

* **线性回归模型**：线性回归是利用数理统计中的回归分析，来确定两种或两种以上变量间相互依赖的定量关系的一种统计分析方法。其基本形式可以表示为 $y = \mathbf{w}^T\mathbf{x} + b$，其中 $\mathbf{w}$ 是权重向量，$\mathbf{x}$ 是特征向量，$b$ 是截距项，$y$ 是预测值。
* ==**权重向量 $\mathbf{w}$**：权重向量 $\mathbf{w} = (w_1, w_2, \ldots, w_n)$ 中的每个元素 $w_i$ 代表了对应特征 $x_i$ 在预测结果 $y$ 中的重要程度或影响力==。

二、几何意义

1. **方向性**：
   * 权重向量 $\mathbf{w}$ 在几何上表示了一个方向，<font color=red>这个方向是特征空间（由 $\mathbf{x}$ 的各个分量构成的空间）中，预测值 $y$ 随特征变化而变化的趋势方向</font>。
   * 当 $\mathbf{x}$ 在这个方向上移动时，$y$ 的变化最为显著，因为 $\mathbf{w}$ 决定了 $y$ 随 $\mathbf{x}$ 变化的速率和方向。

2. **权重大小与重要性**：
   * 权重向量 $\mathbf{w}$ 中各元素 $w_i$ 的绝对值大小，反映了对应特征 $x_i$ 在预测中的重要性或影响力。
   * $|w_i|$ 越大，说明特征 $x_i$ 对 $y$ 的预测值影响越大；反之，$|w_i|$ 越小，说明特征 $x_i$ 的影响力越小。

3. ==**超平面**==：
   * 在多元线性回归中，权重向量 $\mathbf{w}$ 和截距 $b$ 共同定义了一个超平面，该超平面将特征空间划分为两部分，预测值 $y$ 高于或低于这个超平面的值，取决于 $\mathbf{x}$ 在特征空间中的位置。
   * ==这个超平面的法线方向就是权重向量 $\mathbf{w}$ 的方向，它代表了 $y$ 随 $\mathbf{x}$ 变化的最快速率方向==。

4. **投影与距离**：
   * 从几何角度看，线性回归也可以理解为在特征空间中寻找一个超平面，**使得所有观测点到这个超平面的垂直距离（即残差）的平方和最小。**
   * 这个过程实际上是在特征空间中对观测点进行投影，投影方向由权重向量 $\mathbf{w}$ 决定，投影点则位于超平面上。

```python
P10 系列三 线性回归2-最小二乘法 概率视角下 高斯噪声MLE
```

## 二、噪声为高斯分布的 MLE

对于一维的情况，记 $y=w^Tx+\epsilon,\epsilon\sim\mathcal{N}(0,\sigma^2)$（噪声属于高斯分布），那么 $y\sim\mathcal{N}(w^Tx,\sigma^2)$​。代入极大似然估计中：

- 我的理解：y|x;w的意思是指，在参数为w的模型中，y在x的条件下所服从的概率分布。

$$
\begin{align}
L(w)=\log p(Y|X,w)&=\log\prod\limits_{i=1}^Np(y_i|x_i,w)\nonumber\\
&=\sum\limits_{i=1}^N\log(\frac{1}{\sqrt{2\pi\sigma}}e^{-\frac{(y_i-w^Tx_i)^2}{2\sigma^2}})\\
\mathop{argmax}\limits_wL(w)&=\mathop{argmin}\limits_w\sum\limits_{i=1^N}(y_i-w^Tx_i)^2
\end{align}
$$
这个表达式和最小二乘估计得到的结果一样。

 ![image-20240901162957160](/Users/zhihongli/Documents/Course/MachineLearningNotes-master/pic/image-20240901162957160.png)

### 2.1 LSE（最小二乘估计）

**定义与原理**：

* LSE是一种数学优化技术，==通过最小化误差的平方和来寻找数据的最佳函数匹配==。在回归分析中，它用于找到最佳拟合直线（或曲线）以最小化实际观测值与模型预测值之间的差的平方和。
* 最小二乘法可以应用于线性模型，也可以扩展到非线性模型。其核心思想是通过构造一条曲线（或超平面）来拟合原始数据，使得预测值与观测值之差的平方和最小。

**应用**：

* LSE广泛应用于**数据拟合、曲线拟合、回归分析**等领域。
* 在线性回归中，LSE通过求解正规方程（Normal Equation）来找到最佳拟合直线的参数。

### 2.2 MLE（极大似然估计）

**定义与原理**：

* MLE是一种在统计学中估计概率模型参数的方法，即给定一组观测数据，通过最大化观测数据出现的概率（即似然函数）来估计模型的参数。
* ==极大似然估计的原理是认为使得观测数据出现的概率最大的参数就是模型的真实参数==。

**应用**：

* MLE广泛应用于各种概率模型的参数估计中，如线性回归、逻辑回归、时间序列分析等。
* 在实际应用中，常常通过求解似然函数的导数并令其为零来找到参数的极大似然估计值。

### 2.3 LSE与MLE的联系与区别

**联系**：

* 在某些特定条件下，==LSE和MLE是等价的。特别是当误差项服从高斯分布时==，极大化似然函数就等价于极小化误差项的平方和（即LSE）。
* 这意味着，在线性回归等模型中，如果假设误差项服从高斯分布，则通过LSE和MLE得到的参数估计值将是相同的。

```python
P11 (系列三) 线性回归3-正则化-岭回归-频率角度
```

$$
\hat{w}=(X^TX+\lambda\mathbb{I})^{-1}X^TY
$$



![image-20240901192039582](/Users/zhihongli/Documents/Course/MachineLearningNotes-master/pic/image-20240901192039582.png)

 ```python
 P12  (系列三) 线性回归4-正则化-岭回归-贝叶斯角度
 ```



## 三、权重先验也为高斯分布的 MAP

- 特征维度超过数据维度就会不可逆

$$
\hat{w}=(X^TX)^{-1}X^TY
$$



- 取先验分布 $w\sim\mathcal{N}(0,\sigma_0^2)$。于是： 

$$
\begin{align}
\hat{w}=\mathop{argmax}\limits_wp(w|Y)&=\mathop{argmax}\limits_wp(Y|w)p(w)\nonumber\\
&=\mathop{argmax}\limits_w\log p(Y|w)p(w)\nonumber\\
&=\mathop{argmax}\limits_w(\log p(Y|w)+\log p(w))\nonumber\\
&=\mathop{argmin}\limits_w[(y-w^Tx)^2+\frac{\sigma^2}{\sigma_0^2}w^Tw]
\end{align}
$$
这里省略了 $X$，$p(Y)$和 $w$ 没有关系，同时也利用了上面高斯分布的 MLE的结果。

我们将会看到，超参数 $\sigma_0$​的存在和下面会介绍的 Ridge 正则项可以对应，同样的如果将先验分布取为 Laplace 分布，那么就会得到和 L1 正则类似的结果。

![image-20240901200656684](/Users/zhihongli/Documents/Course/MachineLearningNotes-master/pic/image-20240901200656684.png)

- 上述在最后一步，同时乘以2$$\sigma^2$$

## 四、正则化

在实际应用时，==如果样本容量不远远大于样本的特征维度，很可能造成过拟合==，对这种情况，我们有下面三个解决方式：

1.  加数据
2.  特征选择（降低特征维度）如 PCA 算法。
3.  正则化

正则化一般是在损失函数（如上面介绍的最小二乘损失）上加入正则化项（<font color=red>表示模型的复杂度对模型的惩罚</font>），下面我们介绍一般情况下的两种正则化框架。
$$
\begin{align}
L1&:\mathop{argmin}\limits_wL(w)+\lambda||w||_1,\lambda\gt0\\
L2&:\mathop{argmin}\limits_wL(w)+\lambda||w||^2_2,\lambda \gt 0
\end{align}
$$
下面对最小二乘误差分别分析这两者的区别。

### 4.1 L1 Lasso

L1正则化可以引起稀疏解。

从最小化损失的角度看，由于 L1 项求导在0附近的左右导数都不是0，因此更容易取到0解。

从另一个方面看，L1 正则化相当于：
$$
\mathop{argmin}\limits_wL(w)\\
s.t. ||w||_1\lt C
$$
我们已经看到平方误差损失函数在 $w$ 空间是一个椭球，因此上式求解就是椭球和 $||w||_1=C$的切点，因此更容易相切在坐标轴上。

### 4.2 L2 Ridge

$$
\begin{align}
\hat{w}=\mathop{argmin}\limits_wL(w)+\lambda w^Tw&\longrightarrow\frac{\partial}{\partial w}L(w)+2\lambda w=0\nonumber\\
&\longrightarrow2X^TX\hat{w}-2X^TY+2\lambda \hat w=0\nonumber\\
&\longrightarrow \hat{w}=(X^TX+\lambda \mathbb{I})^{-1}X^TY
\end{align}
$$

可以看到，这个正则化参数和前面的 MAP 结果不谋而合。利用2范数进行正则化不仅可以是模型选择 $w$ 较小的参数，同时也避免 $ X^TX$​不可逆的问题。





在统计学和机器学习中，MAP（Maximum A Posteriori Estimation，极大后验估计）和LSE（Least Square Estimation，最小二乘估计）是两种不同的参数估计方法，它们各自有不同的应用场景和优缺点。

### 4.3 MAP（极大后验估计）

MAP是贝叶斯学派中的一种参数估计方法。**其基本原理是，在给定观测数据X的条件下，通过最大化参数w的后验概率P(w|X)来估计w的值**。后验概率P(w|X)可以通过贝叶斯公式计算得到，即P(w|X) = P(X|w) \* P(w) / P(X)，其中P(X|w)是似然函数，P(w)是先验概率，P(X)是观测数据的边缘概率（通常作为常数处理）。因此，MAP估计可以表示为：

$$ \hat{w} = \arg\max_{w} P(w|X) = \arg\max_{w} P(X|w) \cdot P(w) $$

在实际应用中，由于P(X)是常数，所以MAP估计通常简化为最大化似然函数P(X|w)和先验概率P(w)的乘积。MAP估计结合了先验知识和观测数据，能够在一定程度上避免过拟合，提高估计的稳健性。

### LSE（最小二乘估计）

LSE是一种基于最小化误差平方和的参数估计方法。在线性回归模型中，LSE通过最小化预测值与真实值之差的平方和来估计回归系数w。LSE的损失函数L(w)可以表示为：

$$ L(w) = \frac{1}{2} \sum_{i=1}^{n} (w^\top x_i - y_i)^2 $$

其中，$x_i$是第i个观测数据的特征向量，$y_i$是第i个观测数据的真实值，n是观测数据的数量。LSE估计通过求解以下优化问题来得到w的最优解：

$$ \hat{w} = \arg\min_{w} L(w) $$

LSE估计具有计算简单、易于理解的优点，并且在许多实际应用中表现出色。然而，当特征空间的维度大于样本数时，LSE估计可能会受到过拟合的影响。为了解决这个问题，可以通过正则化方法来改进LSE估计，如添加L1或L2正则化项。



## 五、小结

线性回归模型是最简单的模型，但是麻雀虽小，五脏俱全，在这里，我们利用最小二乘误差得到了闭式解。同时也发现，在噪声为高斯分布的时候，MLE 的解等价于最小二乘误差，而增加了正则项后，最小二乘误差加上 L2 正则项等价于高斯噪声先验下的 MAP解，加上 L1 正则项后，等价于 Laplace 噪声先验。

传统的机器学习方法或多或少都有线性回归模型的影子：

1.  线性模型往往不能很好地拟合数据，因此有三种方案克服这一劣势：
    1.  对特征的维数进行变换，例如多项式回归模型就是在线性特征的基础上加入高次项。
    2.  在线性方程后面加入一个非线性变换，即引入一个非线性的激活函数，典型的有线性分类模型如感知机。
    3.  对于一致的线性系数，我们进行多次变换，这样同一个特征不仅仅被单个系数影响，例如多层感知机（深度前馈网络）。
2.  线性回归在整个样本空间都是线性的，我们修改这个限制，在不同区域引入不同的线性或非线性，例如线性样条回归和决策树模型。
3.  线性回归中使用了所有的样本，但是对数据预先进行加工学习的效果可能更好（所谓的维数灾难，高维度数据更难学习），例如 PCA 算法和流形学习。

