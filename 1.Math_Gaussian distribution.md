---
typora-copy-images-to: ./pic
---

[toc]

# Introduction





- 在统计学和概率论中，==$P(x|k)$ 通常表示在给定条件 $k$ 下，事件 $x$ 发生的条件概率==。条件概率是描述两个或多个事件之间关系的概率，其中一个事件的发生依赖于另一个事件的发生。

- 具体来说，$P(x|k)$ 的定义是：在事件 $k$ 已经发生的条件下，事件 $x$ 发生的概率。这个定义可以用以下公式来表示：

$$P(x|k) = \frac{P(x \cap k)}{P(k)}$$

其中，$P(x \cap k)$ 表示事件 $x$ 和事件 $k$ 同时发生的概率，而 $P(k)$ 表示事件 $k$ 发生的概率。



```python
P1 系列一 绪论-资料介绍
```



![image-20240830160631554](/Users/zhihongli/Documents/Course/MachineLearningNotes-master/pic/image-20240830160631554.png)

 ```python
 P2 系列一 绪论-频率派VS贝叶斯派
 ```



对概率的诠释有两大学派，一种是频率派另一种是贝叶斯派。后面我们对观测集采用下面记号： 

>频率---> 统计机器学习  优化模型  Loss function
>
>贝叶斯  --->  概率图模型  求积分





$$
X_{N\times p}=(x_{1},x_{2},\cdots,x_{N})^{T},x_{i}=(x_{i1},x_{i2},\cdots,x_{ip})^{T}
$$
 这个记号表示有 $N$ 个样本，==每个样本都是 $p$ 维向量==。其中每个观测都是由 $p(x|\theta)$ 生成的。

## 频率派的观点

==$p(x|\theta)$中的 $\theta$ 是一个**常量**==。对于 $N$ 个观测来说观测集的概率为 




$$
p(X|\theta)\mathop{=}\limits _{iid}\prod\limits _{i=1}^{N}p(x_{i}|\theta))
$$
。为了求 $\theta$ 的大小，我们采用最大对数似然MLE的方法：
$$
\theta_{MLE}=\mathop{argmax}\limits _{\theta}\log p(X|\theta)\mathop{=}\limits _{iid}\mathop{argmax}\limits _{\theta}\sum\limits _{i=1}^{N}\log p(x_{i}|\theta)
$$


## 贝叶斯派的观点

![image-20240830192858908](/Users/zhihongli/Documents/Course/MachineLearningNotes-master/pic/image-20240830192858908.png)

- 全概率

$$
P(B) = \sum_{i=1}^{n} P(A_i) P(B|A_i)
$$

- 贝叶斯

$$
P(A_i|B) = \frac{P(A_i) P(B|A_i)}{\sum_{j=1}^{n} P(A_j) P(B|A_j)}
$$






这里，`P(A_i|B)` 表示在事件 $ B $ 已经发生的条件下，事件 $ A_i $ 发生的条件概率。公式的分母是事件 $ B $ 的全概率，即 $ P(B) $，它是通过全概率公式计算得到的。



贝叶斯派认为 $p(x|\theta)$ 中的 $\theta$ 不是一个常量。这个 $\theta$ 满<u>足一个预设的先验的分布</u> $\theta\sim p(\theta)$ 。于是根据贝叶斯定理依赖观测集参数的后验可以写成：
$$
p(\theta|X)=\frac{p(X|\theta)\cdot p(\theta)}{p(X)}=\frac{p(X|\theta)\cdot p(\theta)}{\int\limits _{\theta}p(X|\theta)\cdot p(\theta)d\theta}
$$
为了求 $\theta$​​ 的值，我们要最大化这个参数后验==MAP==：

---

📢注意：这里还有个P(X) 省去，因为在关于$\theta$​的函数中，<font color=red>X相当于常量</font>

---



$$
\theta_{MAP}=\mathop{argmax}\limits _{\theta}p(\theta|X)=\mathop{argmax}\limits _{\theta}p(X|\theta)\cdot p(\theta)
$$
其中第二个等号是由于分母和 $\theta$ 没有关系。求解这个 $\theta$ 值后计算
$$
\frac{p(X|\theta)\cdot p(\theta)}{\int\limits _{\theta}p(X|\theta)\cdot p(\theta)d\theta}
$$





，就得到了参数的后验概率。其中 $p(X|\theta)$​ 叫似然，是我们的模型分布。得到了参数的后验分布后，我们可以将这个分布用于预测==贝叶斯预测==：



---

注意：<font color=red>边缘概率</font>   应为p(x,t|X)=p(x|t,X)p(t|X)    联合概率密度与条件概率的结合

---

$$
p(x_{new}|X)=\int\limits _{\theta}p(x_{new}|\theta)\cdot p(\theta|X)d\theta
$$
 其中积分中的被乘数是模型，乘数是后验分布。

## 小结

频率派和贝叶斯派分别给出了一系列的机器学习算法。频率派的观点导出了一系列的统计机器学习算法而贝叶斯派导出了概率图理论。在应用频率派的 MLE 方法时最优化理论占有重要地位。而贝叶斯派的算法无论是后验概率的建模还是应用这个后验进行推断时积分占有重要地位。因此采样积分方法如 MCMC 有很多应用。



```python
import transformers
import datasets
import huggingface_hub

transformers.__version__, datasets.__version__, huggingface_hub.__version__
```





# MathBasics

- ==幂集P(R)指原集合中所有的子集（包括全集和空集）构成的集族==。

- 群：一个集合对二元运算封闭，且有单位元、逆元，满足结合律

- 阿贝尔群：满足交换律的群

- 环：阿贝尔群+乘法

- 域：若逆运算也封闭，则称为域or代数，相当于对除法也封闭了。 



---



- 在概率统计理论中，如果变量序列或者其他随机变量有==相同的概率分布==，并且互相独立，那么这些随机变量是独立同分布(iid)
- [【概率论与数理统计】一个视频让你明白分布函数，概率密度函数，分布律，联合概率密度，联合分布函数，联合分布律，边缘概率密度，边缘分布函数都是什么意义和概念_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV16L4y1q7MW/?spm_id_from=333.788.recommend_more_video.7&vd_source=81e5007efea018d7c2e8c28374fcdf34)  理解容易
- [如何通俗地解释协方差｜马同学图解数学_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1gY4y187TL/?spm_id_from=333.337.search-card.all.click&vd_source=81e5007efea018d7c2e8c28374fcdf34)  理解容易

- [如何用概率论解决真实问题？用随机变量去建模，最大的难题是相关关系_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1Xs4y1z72n/?spm_id_from=333.788&vd_source=81e5007efea018d7c2e8c28374fcdf34) 理解难度高

- [卡方分布 （chi-square distribution） - 统计学_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1aa4y1v7Pf/?spm_id_from=333.337.search-card.all.click&vd_source=81e5007efea018d7c2e8c28374fcdf34) 容易理解

## 二阶中心矩

二阶中心矩是数据与其均值之差的平方的平均值。对于一组数据 $X = \{x_1, x_2, \ldots, x_n\}$，其二阶中心矩 $M_2$ 的 LaTeX 公式为：

$$
M_2 = \frac{1}{n} \sum_{i=1}^{n} (x_i - \mu)^2
$$


其中，$\mu$ 是数据的均值，即 $\mu = \frac{1}{n} \sum_{i=1}^{n} x_i$。但注意，在二阶中心矩的严格定义中，我们通常使用总体均值 $\mu$。然而，在实际应用中，当我们只有样本数据时，我们可能会用样本均值 $\bar{x}$ 来代替 $\mu$。

## 样本方差 

样本方差是样本数据与其样本均值之差的平方的平均值，==但通常我们会乘以一个因子 $\frac{n}{n-1}$（称为贝塞尔校正因子）来得到无偏估计==。对于一组样本数据 $X = \{x_1, x_2, \ldots, x_n\}$，其样本方差 $s^2$​ 的 LaTeX 公式为：
$$
s^2 = \frac{1}{n-1} \sum_{i=1}^{n} (x_i - \bar{x})^2
$$



其中，$\bar{x}$ 是样本均值，即 $\bar{x} = \frac{1}{n} \sum_{i=1}^{n} x_i$​。



总结

- 二阶中心矩通常使用总体均值来计算，但在只有样本数据时，也可以用样本均值来近似。
- 样本方差是二阶中心矩在样本数据上的具体应用，但乘以了一个贝塞尔校正因子 $\frac{n}{n-1}$ 以得到无偏估计。
- LaTeX 公式清晰地展示了这些统计量的数学表达式。

```python
P3 (系列二) 数学基础-概率-高斯分布1 极大似然估计
```



## 高斯分布

### 一维情况 MLE

- 一维情况 最大似然估计

高斯分布（Gaussian distribution）和正态分布（Normal distribution）在统计学和概率论中是同一个概念的不同称呼。

当说一个随机变量 $Y$ 服从正态分布（或高斯分布），并且其分布的参数是<font color=red>均值（mean）$\mu$ 和标准差（standard deviation）$\sigma$ 时</font>，我们写作 $Y \sim N(\mu, \sigma^2)$。注意，虽然你写的是 $Y \sim N(\mu, \sigma)$，但通常标准差 $\sigma$ 是以平方的形式 $\sigma^2$ 出现在正态分布的表示中，==以表示方差（variance==）。<font color=blue>方差是标准差的平方，它衡量了数据分布的离散程度</font>。

 

正态分布的概率密度函数（Probability Density Function, PDF）为：

$$f(y|\mu,\sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(y-\mu)^2}{2\sigma^2}}$$

 

正态分布之所以重要，是因为很多自然现象和社会现象都近似地服从正态分布，或者可以通过适当的变换（如对数变换）转换为正态分布。此外，中心极限定理也说明了在许多独立同分布的随机变量之和的分布趋向于正态分布。

MLE:  maximum liklihood estimation  ==最大似然估计==

高斯分布在机器学习中占有举足轻重的作用。在 MLE 方法中：

$$
\theta=(\mu,\Sigma)=(\mu,\sigma^{2}),\theta_{MLE}=\mathop{argmax}\limits _{\theta}\log p(X|\theta)\mathop{=}\limits _{iid}\mathop{argmax}\limits _{\theta}\sum\limits _{i=1}^{N}\log p(x_{i}|\theta)
$$
一般地，高斯分布的概率密度函数PDF写为：

$$
p(x|\mu,\Sigma)=\frac{1}{(2\pi)^{p/2}|\Sigma|^{1/2}}e^{-\frac{1}{2}(x-\mu)^{T}\Sigma^{-1}(x-\mu)}
$$
带入 MLE 中我们考虑一维的情况

$$
\log p(X|\theta)=\sum\limits _{i=1}^{N}\log p(x_{i}|\theta)=\sum\limits _{i=1}^{N}\log\frac{1}{\sqrt{2\pi}\sigma}\exp(-(x_{i}-\mu)^{2}/2\sigma^{2})
$$
首先对 $\mu$ 的极值可以得到 ：
$$
\mu_{MLE}=\mathop{argmax}\limits _{\mu}\log p(X|\theta)=\mathop{argmax}\limits _{\mu}\sum\limits _{i=1}^{N}(x_{i}-\mu)^{2}
$$
 于是：
$$
\frac{\partial}{\partial\mu}\sum\limits _{i=1}^{N}(x_{i}-\mu)^{2}=0\longrightarrow\mu_{MLE}=\frac{1}{N}\sum\limits _{i=1}^{N}x_{i}
$$
---

$$
\mu_{MLE}=\frac{1}{N}\sum\limits _{i=1}^{N}x_{i}
$$

$$
\sigma_{MLE}^{2}=\frac{1}{N}\sum\limits _{i=1}^{N}(x_{i}-\mu)^{2}
$$

---

其次对 $\theta$ 中的另一个参数 $\sigma$​ ，有：
$$
\begin{align}
\sigma_{MLE}=\mathop{argmax}\limits _{\sigma}\log p(X|\theta)&=\mathop{argmax}\limits _{\sigma}\sum\limits _{i=1}^{N}[-\log\sigma-\frac{1}{2\sigma^{2}}(x_{i}-\mu)^{2}]\nonumber\\
&=\mathop{argmin}\limits _{\sigma}\sum\limits _{i=1}^{N}[\log\sigma+\frac{1}{2\sigma^{2}}(x_{i}-\mu)^{2}]
\end{align}
$$
于是：
$$
\frac{\partial}{\partial\sigma}\sum\limits _{i=1}^{N}[\log\sigma+\frac{1}{2\sigma^{2}}(x_{i}-\mu)^{2}]=0\longrightarrow\sigma_{MLE}^{2}=\frac{1}{N}\sum\limits _{i=1}^{N}(x_{i}-\mu)^{2}
$$
值得注意的是，上面的推导中，首先对 $\mu$ 求 MLE， 然后利用这个结果求 $\sigma_{MLE}$ ，因此可以预期的是对数据集求期望时 $\mathbb{E}_{\mathcal{D}}[\mu_{MLE}]$ 是无偏差的：
$$
\mathbb{E}_{\mathcal{D}}[\mu_{MLE}]=\mathbb{E}_{\mathcal{D}}[\frac{1}{N}\sum\limits _{i=1}^{N}x_{i}]=\frac{1}{N}\sum\limits _{i=1}^{N}\mathbb{E}_{\mathcal{D}}[x_{i}]=\mu
$$
但是当对 $\sigma_{MLE}$ 求 期望的时候由于使用了单个数据集的 $\mu_{MLE}$，因此对所有数据集求期望的时候我们会发现 $\sigma_{MLE}$​ 是 有偏的：



---

📢 下面公式推导  


$$
\sigma_{MLE}^{2} =\frac{1}{N}\sum\limits _{i=1}^{N}(x_{i}-\mu_{MLE})^{2}
$$

- 有偏的原因是因为用样本均值代替总体均值，假如本身系统设计时均值是已知的。只用mle算方差的话，除以n也还是无偏估计


---

$$
\begin{align}
\mathbb{E}_{\mathcal{D}}[\sigma_{MLE}^{2}]&=\mathbb{E}_{\mathcal{D}}[\frac{1}{N}\sum\limits _{i=1}^{N}(x_{i}-\mu_{MLE})^{2}]=\mathbb{E}_{\mathcal{D}}[\frac{1}{N}\sum\limits _{i=1}^{N}(x_{i}^{2}-2x_{i}\mu_{MLE}+\mu_{MLE}^{2})\nonumber
\\&=\mathbb{E}_{\mathcal{D}}[\frac{1}{N}\sum\limits _{i=1}^{N}x_{i}^{2}-\mu_{MLE}^{2}]=\mathbb{E}_{\mathcal{D}}[\frac{1}{N}\sum\limits _{i=1}^{N}x_{i}^{2}-\mu^{2}+\mu^{2}-\mu_{MLE}^{2}]\nonumber\\
&= \mathbb{E}_{\mathcal{D}}[\frac{1}{N}\sum\limits _{i=1}^{N}x_{i}^{2}-\mu^{2}]-\mathbb{E}_{\mathcal{D}}[\mu_{MLE}^{2}-\mu^{2}]=\sigma^{2}-(\mathbb{E}_{\mathcal{D}}[\mu_{MLE}^{2}]-\mu^{2})\nonumber\\&=\sigma^{2}-(\mathbb{E}_{\mathcal{D}}[\mu_{MLE}^{2}]-\mathbb{E}_{\mathcal{D}}^{2}[\mu_{MLE}])=\sigma^{2}-Var[\mu_{MLE}]\nonumber\\&=\sigma^{2}-Var[\frac{1}{N}\sum\limits _{i=1}^{N}x_{i}]=\sigma^{2}-\frac{1}{N^{2}}\sum\limits _{i=1}^{N}Var[x_{i}]=\frac{N-1}{N}\sigma^{2}
\end{align}
$$
所以：
$$
\hat{\sigma}^{2}=\frac{1}{N-1}\sum\limits _{i=1}^{N}(x_{i}-\mu)^{2}
$$

![image-20240831154912664](/Users/zhihongli/Documents/Course/MachineLearningNotes-master/pic/image-20240831154912664.png)

```python
P4 (系列二) 数学基础-概率-高斯分布2 极大似然估计 有偏vs无偏
```





![image-20240831161701741](/Users/zhihongli/Documents/Course/MachineLearningNotes-master/pic/image-20240831161701741.png)

### 多维情况

```python
P5 (系列二) 数学基础-概率-高斯分布3 从概率密度角度观察
```



- [多维高斯分布（Multivariate Gaussian Distribution，MGD）的采样过程是什么样的？-CSDN博客](https://blog.csdn.net/mieshizhishou/article/details/141572412)

- pdf: probability density function  即为概率密度函数

多维高斯分布表达式为：
$$
p(x|\mu,\Sigma)=\frac{1}{(2\pi)^{p/2}|\Sigma|^{1/2}}e^{-\frac{1}{2}(x-\mu)^{T}\Sigma^{-1}(x-\mu)}
$$
其中 $x,\mu\in\mathbb{R}^{p},\Sigma\in\mathbb{R}^{p\times p}$ ，==$\Sigma$ 为协方差矩阵==，一般而言也是半正定矩阵。这里我们只考虑正定矩阵。首先我们处理指数上的数字，指数上的数字可以记为 $x$ 和 $\mu$ 之间的马氏距离。对于对称的协方差矩阵可进行特征值分解，

- 这里二次型U当作是正交矩阵，那么u的逆等于u的转置

$\Sigma=U\Lambda U^{T}=(u_{1},u_{2},\cdots,u_{p})diag(\lambda_{i})(u_{1},u_{2},\cdots,u_{p})^{T}=\sum\limits _{i=1}^{p}u_{i}\lambda_{i}u_{i}^{T}$​ ，于是：
$$
\Sigma^{-1}=\sum\limits _{i=1}^{p}u_{i}\frac{1}{\lambda_{i}}u_{i}^{T}
$$

$$
\Delta=(x-\mu)^{T}\Sigma^{-1}(x-\mu)=\sum\limits _{i=1}^{p}(x-\mu)^{T}u_{i}\frac{1}{\lambda_{i}}u_{i}^{T}(x-\mu)=\sum\limits _{i=1}^{p}\frac{y_{i}^{2}}{\lambda_{i}}
$$

我们注意到 $y_{i}$ 是 $x-\mu$ 在特征向量 $u_{i}$ 上的投影长度，因此上式子就是 $\Delta$ 取不同值时的同心椭圆。


$$
(x-\mu)^{T}\Sigma^{-1}(x-\mu)   即为x与\mu马式距离
$$




![image-20240831171023362](/Users/zhihongli/Documents/Course/MachineLearningNotes-master/pic/image-20240831171023362.png)

```python
P6 (系列二) 数学基础-概率-高斯分布4 局限性
```



下面我们看多维高斯模型在实际应用时的两个问题  ==高斯定理的局限性==

1. 参数 $\Sigma,\mu$ 的自由度为 $O(p^{2})$ 对于维度很高的数据其自由度太高。解决方案：高自由度的来源是 $\Sigma$ 有 $\frac{p(p+1)}{2}$ 个自由参数   --->  由于是 PxP 对称矩阵, 首先 (PxP-P) /2  将矩阵对称轴挖去，且将对称的部分除以2，然后加上对称轴。 

   

   可以假设其是对角矩阵，**甚至在各向同性假设中假设其对角线上的元素都相同**。前一种的算法有 Factor Analysis，后一种有概率 PCA(p-PCA) 。

2. 第二个问题是单个高斯分布是单峰的，对有多个峰的数据分布不能得到好的结果。解决方案：高斯混合GMM 模型（多个高斯进行混合）。

```python
P7 系列二 数学基础-概率-高斯分布5- 求边缘概率及条件概率
```



下面对多维高斯分布的常用定理进行介绍。

我们记 $x=(x_1, x_2,\cdots,x_p)^T=(x_{a,m\times 1}, x_{b,n\times1})^T,\mu=(\mu_{a,m\times1}, \mu_{b,n\times1}),\Sigma=\begin{pmatrix}\Sigma_{aa}&\Sigma_{ab}\\\Sigma_{ba}&\Sigma_{bb}\end{pmatrix}$，已知 $x\sim\mathcal{N}(\mu,\Sigma)$​。



首先是一个高斯分布的定理：

>   定理：已知 $x\sim\mathcal{N}(\mu,\Sigma), y\sim Ax+b$，==那么 $y\sim\mathcal{N}(A\mu+b, A\Sigma A^T)$==。
>
>   证明：$\mathbb{E}[y]=\mathbb{E}[Ax+b]=A\mathbb{E}[x]+b=A\mu+b$，$Var[y]=Var[Ax+b]=Var[Ax]=A\cdot Var[x]\cdot A^T$。

下面利用这个定理得到 $p(x_a),p(x_b),p(x_a|x_b),p(x_b|x_a)$ 这四个量。

1. $x_a=\begin{pmatrix}\mathbb{I}_{m\times m}&\mathbb{O}_{m\times n})\end{pmatrix}\begin{pmatrix}x_a\\x_b\end{pmatrix}$，代入定理中得到：   其中$${I}_{m\times m}$$为单位矩阵

   
   $$
   \mathbb{E}[x_a]=\begin{pmatrix}\mathbb{I}&\mathbb{O}\end{pmatrix}\begin{pmatrix}\mu_a\\\mu_b\end{pmatrix}=\mu_a\\
   Var[x_a]=\begin{pmatrix}\mathbb{I}&\mathbb{O}\end{pmatrix}\begin{pmatrix}\Sigma_{aa}&\Sigma_{ab}\\\Sigma_{ba}&\Sigma_{bb}\end{pmatrix}\begin{pmatrix}\mathbb{I}\\\mathbb{O}\end{pmatrix}=\Sigma_{aa}
   $$
   所以 $x_a\sim\mathcal{N}(\mu_a,\Sigma_{aa})$。

2. 同样的，$x_b\sim\mathcal{N}(\mu_b,\Sigma_{bb})$​。

---

下面开始条件概率

1. 对于两个条件概率，我们引入三个量：（下面三个都是构造性变量）
   $$
   x_{b\cdot a}=x_b-\Sigma_{ba}\Sigma_{aa}^{-1}x_a\\
   \mu_{b\cdot a}=\mu_b-\Sigma_{ba}\Sigma_{aa}^{-1}\mu_a\\
   \Sigma_{bb\cdot a}=\Sigma_{bb}-\Sigma_{ba}\Sigma_{aa}^{-1}\Sigma_{ab} 
   $$
   特别的，最后一个式子叫做 $\Sigma_{bb}$ 的 Schur Complementary。可以看到：
   $$
   x_{b\cdot a}=\begin{pmatrix}-\Sigma_{ba}\Sigma_{aa}^{-1}&\mathbb{I}_{n\times n}\end{pmatrix}\begin{pmatrix}x_a\\x_b\end{pmatrix}
   $$
   所以： 下面都是套公式
   $$
   \mathbb{E}[x_{b\cdot a}]=\begin{pmatrix}-\Sigma_{ba}\Sigma_{aa}^{-1}&\mathbb{I}_{n\times n}\end{pmatrix}\begin{pmatrix}\mu_a\\\mu_b\end{pmatrix}=\mu_{b\cdot a}\\
   Var[x_{b\cdot a}]=\begin{pmatrix}-\Sigma_{ba}\Sigma_{aa}^{-1}&\mathbb{I}_{n\times n}\end{pmatrix}\begin{pmatrix}\Sigma_{aa}&\Sigma_{ab}\\\Sigma_{ba}&\Sigma_{bb}\end{pmatrix}\begin{pmatrix}-\Sigma_{aa}^{-1}\Sigma_{ba}^T\\\mathbb{I}_{n\times n}\end{pmatrix}=\Sigma_{bb\cdot a}  -->  由（32）可知
   $$
   利用这三个量可以得到 $x_b=x_{b\cdot a}+\Sigma_{ba}\Sigma_{aa}^{-1}x_a$。因此：
   $$
   \mathbb{E}[x_b|x_a]=\mu_{b\cdot a}+\Sigma_{ba}\Sigma_{aa}^{-1}x_a
   $$

   $$
   Var[x_b|x_a]=\Sigma_{bb\cdot a}
   $$

   这里同样用到了定理。

   ![image-20240831202523145](/Users/zhihongli/Documents/Course/MachineLearningNotes-master/pic/image-20240831202523145.png)

2. 同样：
   $$
   x_{a\cdot b}=x_a-\Sigma_{ab}\Sigma_{bb}^{-1}x_b\\
   \mu_{a\cdot b}=\mu_a-\Sigma_{ab}\Sigma_{bb}^{-1}\mu_b\\
   \Sigma_{aa\cdot b}=\Sigma_{aa}-\Sigma_{ab}\Sigma_{bb}^{-1}\Sigma_{ba}
   $$
   所以：

   
   $$
   \mathbb{E}[x_a|x_b]=\mu_{a\cdot b}+\Sigma_{ab}\Sigma_{bb}^{-1}x_b
   $$

   $$
   Var[x_a|x_b]=\Sigma_{aa\cdot b}
   $$

---

- 注意，此时求的是Xb关于Xa的条件概率分布，==因此这里认为Xa已知==
- Xb跟Xa关系的那个式子，就算是体现了Xa与Xb的一个条件关系，对给定的Xa，有固定映射的Xb，所以此时E（Xb）为给定条件Xa下的E（Xb），亦即E（Xb|Xa）了。

![image-20240831203922474](/Users/zhihongli/Documents/Course/MachineLearningNotes-master/pic/image-20240831203922474.png)

```python
P8 系列二 数学基础-概率-高斯分布6 求联合概率分布
```



下面利用上边四个量，求解线性模型：

定理  已知 $x\sim\mathcal{N}(\mu,\Sigma), y\sim Ax+b$，==那么 $y\sim\mathcal{N}(A\mu+b, A\Sigma A^T)$​==。



>   已知：$p(x)=\mathcal{N}(\mu,\Lambda^{-1}),p(y|x)=\mathcal{N}(Ax+b,L^{-1})$，求解：$p(y),p(x|y)$。
>
>   解：==令 $y=Ax+b+\epsilon,\epsilon\sim\mathcal{N}(0,L^{-1})$，==所以 $\mathbb{E}[y]=\mathbb{E}[Ax+b+\epsilon]=A\mu+b$，$Var[y]=A \Lambda^{-1}A^T+L^{-1}$，因此：
>   $$
>   p(y)=\mathcal{N}(A\mu+b,L^{-1}+A\Lambda^{-1}A^T)
>   $$
>   引入 $z=\begin{pmatrix}x\\y\end{pmatrix}$，我们可以得到 $Cov[x,y]=\mathbb{E}[(x-\mathbb{E}[x])(y-\mathbb{E}[y])^T]$。对于这个协方差可以直接计算：
>   $$
>   \begin{align}
>   Cov(x,y)&=\mathbb{E}[(x-\mu)(Ax-A\mu+\epsilon)^T]=\mathbb{E}[(x-\mu)(x-\mu)^TA^T]=Var[x]A^T=\Lambda^{-1}A^T
>   \end{align}
>   $$
>   注意到协方差矩阵的对称性，所以 $p(z)=\mathcal{N}\begin{pmatrix}\mu\\A\mu+b\end{pmatrix},\begin{pmatrix}\Lambda^{-1}&\Lambda^{-1}A^T\\A\Lambda^{-1}&L^{-1}+A\Lambda^{-1}A^T\end{pmatrix})$。根据之前的公式，我们可以得到：
>   $$
>   \mathbb{E}[x|y]=\mu+\Lambda^{-1}A^T(L^{-1}+A\Lambda^{-1}A^T)^{-1}(y-A\mu-b)
>   $$
>
>   $$
>   Var[x|y]=\Lambda^{-1}-\Lambda^{-1}A^T(L^{-1}+A\Lambda^{-1}A^T)^{-1}A\Lambda^{-1}
>   $$
>
>   ![image-20240831210428211](/Users/zhihongli/Documents/Course/MachineLearningNotes-master/pic/image-20240831210428211.png)

